"""
ShopLens End-to-End Review Pipeline

Runs the full scraping pipeline for a product:
  1. Check product cache
  2. Search YouTube reviews
  3. Ingest YouTube reviews
  4. Search blog reviews
  5. Ingest blog reviews
  6. Generate reviews summary
  7. Find marketplace listings

Usage:
  python run_pipeline.py "Samsung Galaxy S25"
  python run_pipeline.py "iPhone 16 Pro" --youtube-limit 5 --blog-limit 3
  python run_pipeline.py "Pixel 9" --skip-marketplace
  python run_pipeline.py "MacBook Pro M4" --db-host localhost
"""

import argparse
import asyncio
import json
import sys
import time
import os

# â”€â”€ ANSI colors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BOLD = "\033[1m"
DIM = "\033[2m"
RED = "\033[91m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
MAGENTA = "\033[95m"
CYAN = "\033[96m"
RESET = "\033[0m"

# Disable colors if not a TTY
if not sys.stdout.isatty():
    BOLD = DIM = RED = GREEN = YELLOW = BLUE = MAGENTA = CYAN = RESET = ""


def header(step: int, total: int, title: str):
    print(f"\n{'â”€' * 70}")
    print(f"{BOLD}{CYAN}[Step {step}/{total}]{RESET} {BOLD}{title}{RESET}")
    print(f"{'â”€' * 70}")


def success(msg: str):
    print(f"  {GREEN}âœ“{RESET} {msg}")


def warning(msg: str):
    print(f"  {YELLOW}âš {RESET} {msg}")


def error(msg: str):
    print(f"  {RED}âœ—{RESET} {msg}")


def info(msg: str):
    print(f"  {DIM}{msg}{RESET}")


def elapsed(start: float) -> str:
    return f"{DIM}({time.time() - start:.1f}s){RESET}"


def print_json_compact(data, indent=4):
    """Print a dict with truncated long strings."""
    def truncate(obj, max_len=200):
        if isinstance(obj, str) and len(obj) > max_len:
            return obj[:max_len] + "..."
        if isinstance(obj, dict):
            return {k: truncate(v, max_len) for k, v in obj.items()}
        if isinstance(obj, list):
            return [truncate(v, max_len) for v in obj]
        return obj

    print(json.dumps(truncate(data), indent=indent, default=str))


# â”€â”€ Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def run_pipeline(
    product_name: str,
    youtube_limit: int = 3,
    blog_limit: int = 2,
    skip_marketplace: bool = False,
    db_host: str | None = None,
):
    """Run the full review scraping pipeline."""

    # â”€â”€ 0. Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # Override DB host before importing session (engine is created at import time)
    if db_host:
        from app.core.config import settings as _settings
        original_url = _settings.DATABASE_URL
        import re as _re
        new_url = _re.sub(r"@[^:]+:", f"@{db_host}:", original_url)
        _settings.DATABASE_URL = new_url
        print(f"{DIM}DB host overridden: {new_url}{RESET}")

    from app.db.session import AsyncSessionLocal
    from app.functions.registry import execute_function
    import app.functions.review_tools  # noqa: F401 â€” triggers @register_function

    total_steps = 6 if skip_marketplace else 7
    pipeline_start = time.time()
    youtube_urls: list[str] = []
    blog_urls: list[str] = []
    ingested_reviews: list[dict] = []
    summary_data: dict = {}
    marketplace_data: dict = {}

    print(f"\n{BOLD}{'â•' * 70}{RESET}")
    print(f"{BOLD}  ShopLens Pipeline â€” {MAGENTA}{product_name}{RESET}")
    print(f"{BOLD}{'â•' * 70}{RESET}")
    print(f"  YouTube limit: {youtube_limit} | Blog limit: {blog_limit} | Marketplace: {'skip' if skip_marketplace else 'yes'}")

    async with AsyncSessionLocal() as db:
        try:
            # â”€â”€ Step 1: Check cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            header(1, total_steps, "Checking product cache")
            t = time.time()

            result = await execute_function(db, "check_product_cache", {
                "product_name": product_name,
            })

            if result.get("error"):
                error(f"Cache check failed: {result['error']}")
            elif result.get("status") == "found":
                product = result.get("product", {})
                reviews = result.get("reviews", [])
                success(f"Found cached: {product.get('name', '?')} (id={product.get('id')}) with {len(reviews)} review(s) {elapsed(t)}")
                for r in reviews:
                    info(f"  â€¢ {r.get('reviewer_name', '?')} â€” {r.get('title', 'untitled')}")

                # Ask whether to re-scrape or use cache
                print(f"\n  {YELLOW}Product already has cached reviews.{RESET}")
                choice = input(f"  Re-scrape anyway? [y/N]: ").strip().lower()
                if choice != "y":
                    print(f"\n  {DIM}Skipping scraping, jumping to summary...{RESET}")
                    # Jump straight to summary (step 6)
                    header(total_steps - (0 if skip_marketplace else 1), total_steps, "Generating reviews summary")
                    t = time.time()
                    summary_data = await execute_function(db, "get_reviews_summary", {
                        "product_name": product_name,
                    })
                    if summary_data.get("error"):
                        error(f"Summary failed: {summary_data['error']} {elapsed(t)}")
                    else:
                        success(f"Summary generated {elapsed(t)}")
                    await db.commit()
                    _print_final_summary(product_name, ingested_reviews, summary_data, marketplace_data, pipeline_start, skip_marketplace)
                    return
            elif result.get("status") == "no_reviews":
                warning(f"Product exists but has no reviews â€” will scrape {elapsed(t)}")
            else:
                info(f"Not cached â€” starting fresh scrape {elapsed(t)}")

            # â”€â”€ Step 2: Search YouTube â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            header(2, total_steps, "Searching YouTube reviews (Firecrawl)")
            t = time.time()

            yt_result = await execute_function(db, "search_youtube_reviews", {
                "product_name": product_name,
                "limit": youtube_limit,
            })

            if yt_result.get("error"):
                error(f"YouTube search failed: {yt_result['error']} {elapsed(t)}")
            elif yt_result.get("status") == "success":
                youtube_urls = yt_result.get("urls", [])
                videos = yt_result.get("videos", [])
                success(f"Found {len(youtube_urls)} YouTube video(s) {elapsed(t)}")
                for v in videos:
                    title = v.get("title", "")
                    url = v.get("url", "")
                    desc = v.get("description", "")
                    info(f"  â€¢ {title or url}")
                    info(f"    {DIM}{url}{RESET}")
                    if desc:
                        info(f"    {DIM}{desc[:100]}{RESET}")
            else:
                warning(f"No YouTube results: {yt_result.get('status')} {elapsed(t)}")

            # â”€â”€ Step 3: Ingest YouTube reviews â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            if youtube_urls:
                header(3, total_steps, f"Ingesting {len(youtube_urls)} YouTube review(s)")
                for i, url in enumerate(youtube_urls, 1):
                    t = time.time()
                    print(f"\n  {BOLD}[{i}/{len(youtube_urls)}]{RESET} {url}")

                    ingest_result = await execute_function(db, "ingest_youtube_review", {
                        "video_url": url,
                        "product_name": product_name,
                    })

                    if ingest_result.get("error"):
                        error(f"Failed: {ingest_result['error']} {elapsed(t)}")
                    elif ingest_result.get("status") == "already_exists":
                        warning(f"Already ingested â€” skipped {elapsed(t)}")
                        ingested_reviews.append(ingest_result)
                    elif ingest_result.get("status") == "success":
                        success(f"Ingested: {ingest_result.get('title', '?')} by {ingest_result.get('reviewer_name', '?')} {elapsed(t)}")
                        info(f"  review_id={ingest_result.get('review_id')} product_id={ingest_result.get('product_id')}")
                        ingested_reviews.append(ingest_result)
                    else:
                        warning(f"Unexpected status: {ingest_result.get('status')} {elapsed(t)}")
            else:
                header(3, total_steps, "Ingesting YouTube reviews")
                warning("No YouTube URLs to ingest â€” skipping")

            # â”€â”€ Step 4: Search blog reviews â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            header(4, total_steps, "Searching blog reviews (Firecrawl)")
            t = time.time()

            blog_result = await execute_function(db, "search_blog_reviews", {
                "product_name": product_name,
                "limit": blog_limit,
            })

            if blog_result.get("error"):
                error(f"Blog search failed: {blog_result['error']} {elapsed(t)}")
            elif blog_result.get("status") == "success":
                blog_urls = blog_result.get("urls", [])
                articles = blog_result.get("articles", [])
                success(f"Found {len(blog_urls)} blog article(s) {elapsed(t)}")
                for a in articles:
                    title = a.get("title", "")
                    url = a.get("url", "")
                    desc = a.get("description", "")
                    info(f"  â€¢ {title or url}")
                    info(f"    {DIM}{url}{RESET}")
                    if desc:
                        info(f"    {DIM}{desc[:100]}{RESET}")
            else:
                warning(f"No blog results: {blog_result.get('status')} {elapsed(t)}")

            # â”€â”€ Step 5: Ingest blog reviews â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            if blog_urls:
                header(5, total_steps, f"Ingesting {len(blog_urls)} blog review(s)")
                for i, url in enumerate(blog_urls, 1):
                    t = time.time()
                    print(f"\n  {BOLD}[{i}/{len(blog_urls)}]{RESET} {url}")

                    ingest_result = await execute_function(db, "ingest_blog_review", {
                        "url": url,
                        "product_name": product_name,
                    })

                    if ingest_result.get("error"):
                        error(f"Failed: {ingest_result['error']} {elapsed(t)}")
                    elif ingest_result.get("status") == "already_exists":
                        warning(f"Already ingested â€” skipped {elapsed(t)}")
                        ingested_reviews.append(ingest_result)
                    elif ingest_result.get("status") == "success":
                        success(f"Ingested: {ingest_result.get('title', '?')} by {ingest_result.get('reviewer_name', '?')} {elapsed(t)}")
                        info(f"  review_id={ingest_result.get('review_id')} product_id={ingest_result.get('product_id')}")
                        ingested_reviews.append(ingest_result)
                    else:
                        warning(f"Unexpected status: {ingest_result.get('status')} {elapsed(t)}")
            else:
                header(5, total_steps, "Ingesting blog reviews")
                warning("No blog URLs to ingest â€” skipping")

            # â”€â”€ Step 6: Generate summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            step_num = 6
            header(step_num, total_steps, "Generating reviews summary")
            t = time.time()

            summary_data = await execute_function(db, "get_reviews_summary", {
                "product_name": product_name,
            })

            if summary_data.get("error"):
                error(f"Summary failed: {summary_data['error']} {elapsed(t)}")
            elif summary_data.get("status") in ("not_found", "no_reviews"):
                warning(f"No reviews to summarize: {summary_data['status']} {elapsed(t)}")
            else:
                success(f"Summary generated for {summary_data.get('total_reviews', '?')} review(s) {elapsed(t)}")

            # â”€â”€ Step 7: Marketplace listings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            if not skip_marketplace:
                header(7, total_steps, "Finding marketplace listings")
                t = time.time()

                marketplace_data = await execute_function(db, "find_marketplace_listings", {
                    "product_name": product_name,
                    "count_per_marketplace": 3,
                })

                if marketplace_data.get("error"):
                    error(f"Marketplace search failed: {marketplace_data['error']} {elapsed(t)}")
                elif marketplace_data.get("status") in ("success", "partial"):
                    amazon = marketplace_data.get("amazon", [])
                    ebay = marketplace_data.get("ebay", [])
                    success(f"Found {len(amazon)} Amazon + {len(ebay)} eBay listing(s) {elapsed(t)}")
                else:
                    warning(f"No listings found: {marketplace_data.get('status')} {elapsed(t)}")

            # â”€â”€ Commit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            await db.commit()

        except Exception as e:
            await db.rollback()
            print(f"\n{RED}{BOLD}Pipeline error:{RESET} {e}")
            raise

    # â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    _print_final_summary(product_name, ingested_reviews, summary_data, marketplace_data, pipeline_start, skip_marketplace)


def _print_final_summary(
    product_name: str,
    ingested_reviews: list[dict],
    summary_data: dict,
    marketplace_data: dict,
    pipeline_start: float,
    skip_marketplace: bool,
):
    """Print the final formatted summary."""

    total_time = time.time() - pipeline_start

    print(f"\n\n{'â•' * 70}")
    print(f"{BOLD}{MAGENTA}  PIPELINE RESULTS â€” {product_name}{RESET}")
    print(f"{'â•' * 70}")

    # â”€â”€ Product info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    product = summary_data.get("product", {})
    if product:
        print(f"\n  {BOLD}Product:{RESET} {product.get('name', product_name)}")
        if product.get("brand"):
            print(f"  {BOLD}Brand:{RESET}   {product['brand']}")
        if product.get("category"):
            print(f"  {BOLD}Category:{RESET} {product['category']}")

    # â”€â”€ Reviews ingested â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_reviews = summary_data.get("total_reviews", len(ingested_reviews))
    print(f"\n  {BOLD}Reviews in DB:{RESET} {total_reviews}")

    # â”€â”€ Per-reviewer summaries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    reviewer_summaries = summary_data.get("reviewer_summaries", [])
    if reviewer_summaries:
        print(f"\n  {BOLD}{CYAN}â”€â”€ Reviewer Summaries â”€â”€{RESET}")
        for rs in reviewer_summaries:
            platform_icon = "ðŸŽ¬" if rs.get("platform") == "youtube" else "ðŸ“"
            print(f"\n  {BOLD}{platform_icon} {rs.get('reviewer_name', '?')}{RESET}")
            if rs.get("url"):
                print(f"  {DIM}{rs['url']}{RESET}")
            summary_text = rs.get("summary", "")
            # Word-wrap the summary at ~65 chars
            _print_wrapped(summary_text, indent=4, width=65)

    # â”€â”€ Overall summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    overall = summary_data.get("overall_summary", "")
    if overall:
        print(f"\n  {BOLD}{CYAN}â”€â”€ Overall Summary â”€â”€{RESET}")
        _print_wrapped(overall, indent=4, width=65)

    # â”€â”€ Pros & Cons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pros = summary_data.get("common_pros", [])
    cons = summary_data.get("common_cons", [])
    if pros or cons:
        print(f"\n  {BOLD}{CYAN}â”€â”€ Consensus â”€â”€{RESET}")
    if pros:
        print(f"\n  {GREEN}{BOLD}Pros:{RESET}")
        for p in pros:
            print(f"    {GREEN}+{RESET} {p}")
    if cons:
        print(f"\n  {RED}{BOLD}Cons:{RESET}")
        for c in cons:
            print(f"    {RED}-{RESET} {c}")

    # â”€â”€ Marketplace listings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not skip_marketplace and marketplace_data:
        amazon = marketplace_data.get("amazon", [])
        ebay = marketplace_data.get("ebay", [])
        if amazon or ebay:
            print(f"\n  {BOLD}{CYAN}â”€â”€ Where to Buy â”€â”€{RESET}")
        if amazon:
            print(f"\n  {YELLOW}{BOLD}Amazon:{RESET}")
            for item in amazon:
                price = item.get("price", "N/A")
                print(f"    â€¢ {item.get('title', '?')} â€” {BOLD}{price}{RESET}")
                if item.get("url"):
                    print(f"      {DIM}{item['url']}{RESET}")
        if ebay:
            print(f"\n  {YELLOW}{BOLD}eBay:{RESET}")
            for item in ebay:
                price = item.get("price", "N/A")
                print(f"    â€¢ {item.get('title', '?')} â€” {BOLD}{price}{RESET}")
                if item.get("url"):
                    print(f"      {DIM}{item['url']}{RESET}")

    # â”€â”€ Timing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'â”€' * 70}")
    print(f"  {BOLD}Total time:{RESET} {total_time:.1f}s")
    print(f"{'â•' * 70}\n")


def _print_wrapped(text: str, indent: int = 4, width: int = 65):
    """Print text with word wrapping."""
    prefix = " " * indent
    words = text.split()
    line = prefix
    for word in words:
        if len(line) + len(word) + 1 > width + indent:
            print(line)
            line = prefix + word
        else:
            line = line + " " + word if line.strip() else prefix + word
    if line.strip():
        print(line)


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="ShopLens â€” Run the full review scraping pipeline for a product",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_pipeline.py "Samsung Galaxy S25"
  python run_pipeline.py "iPhone 16 Pro" --youtube-limit 5 --blog-limit 3
  python run_pipeline.py "Pixel 9" --skip-marketplace
  python run_pipeline.py "MacBook Pro M4" --db-host localhost
        """,
    )
    parser.add_argument("product_name", help="Name of the product to research")
    parser.add_argument("--youtube-limit", type=int, default=3, help="Max YouTube videos to find (default: 3)")
    parser.add_argument("--blog-limit", type=int, default=2, help="Max blog articles to find (default: 2)")
    parser.add_argument("--skip-marketplace", action="store_true", help="Skip marketplace listing search")
    parser.add_argument("--db-host", type=str, default=None, help="Override database hostname (e.g. 'localhost' when running outside Docker)")

    args = parser.parse_args()

    asyncio.run(run_pipeline(
        product_name=args.product_name,
        youtube_limit=args.youtube_limit,
        blog_limit=args.blog_limit,
        skip_marketplace=args.skip_marketplace,
        db_host=args.db_host,
    ))


if __name__ == "__main__":
    main()
